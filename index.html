<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">

  <title>OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving</title>
  <link rel="icon" type="image/x-icon" href="static/images/oasim_icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <!-- <script defer src="static/js/fontawesome.all.min.js"></script> -->
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://kit.fontawesome.com/dcd6d05807.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/marked/9.1.0/marked.min.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=xB3V4xIAAAAJ" target="_blank">Guohang Yan</a>,
              </span>
              <span class="author-block">
                <a href="mailto:pijiahao@pjlab.org.cn" target="_blank">Jiahao Pi
                  </a>,
              </span>
              <span class="author-block">
                <a href="mailto:luozhaotong@pjlab.org.cn" target="_blank">Zhaotong Luo</a>,
              </span>
              <span class="author-block">
                <a href="mailto:doumin@pjlab.org.cn" target="_blank">Min Dou</a>,
              </span>
              <span class="author-block">
                <a href="https://ventusff.github.io/" target="_blank">Jianfei Guo</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=AGPz8C4AAAAJ&hl=en" target="_blank">Nianchen Deng</a>,
              </span><br>
              <span class="author-block">
                <a href="mailto:huangqiusheng@pjlab.org.cn" target="_blank">Qiusheng Huang</a>,
              </span>
              <span>
                <a href="https://faculty.ecnu.edu.cn/_s16/hl2/main.psp" target="_blank">
                  Daocheng Fu</a>,
              </span>
              <span>
                <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ" target="_blank">Licheng Wen</a>,
              </span>
              <span>
                <a href="https://scholar.google.com/citations?user=vIU6eHYAAAAJ" target="_blank">
                  Daocheng Fu</a>,
              </span>
              <span>
                <a href="https://scholar.google.com/citations?user=v9CvBeUAAAAJ" target="_blank">Xinyu Cai</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=K0PpvLkAAAAJ" target="_blank">Botian
                  Shi</a> <sup><i class="fa-regular fa-envelope fa-shake"></i></sup>
              </span>
              
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"> Shanghai AI Laboratory, Shanghai, China<br>
              </span>
              <span class="eql-cntrb">
                <small>
                  <br><sup><i class="fa-regular fa-envelope fa-shake"></i></sup> Corresponding author
                </small>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/PJLab-ADG/OASim" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2309.16292" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <center>
          <video poster="" id="tree" autoplay controls muted loop height="100%" width="80%">
            <!-- Your video here -->
            <source src="static/videos/fore-back_ground.mp4" type="video/mp4">
          </video>
        </center>
          <h2 class="subtitle has-text-centered">
            OASim focuses on generating new and highly customizable autonomous driving data through neural implicit 
            reconstruction and rendering techniques. This technology has a wealth of applications, such as large-scale data and scene generation, corner case generation, autonomous driving closed-loop training, autonomous driving stack testing, etc.
          </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->




  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              <b>
                With deep learning and computer vision technology development, autonomous driving provides new solutions to improve traffic safety and efficiency. The importance of building high-quality datasets is self-evident, especially with the rise of end-to-end autonomous driving algorithms in recent years. Data plays a core role in the algorithm closed-loop system. However, collecting real world data is expensive, time-consuming, and unsafe. With the development of implicit rendering technology and in-depth research on how to use generative models to produce data at scale, we propose OASim, an autonomous driving data generator based on neural implicit rendering and reconstruction for highly customized data generation. 
                It has the following characteristics: (1) High-quality scene reconstruction through neural implicit surface reconstruction technology. (2) Trajectory editing of the ego vehicle and participating vehicles. (3) Rich vehicle model library that can be freely selected and inserted to the scene. (4) Rich sensors model library where you can select specified sensors to generate data. (5) A highly customizable data generation system can generate data according to user needs. We demonstrate the high quality and fidelity of the generated data through perception performance evaluation on the Carla simulator and real-world data acquisition.
              </b>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="Framework">
    <div class="container is-max-desktop">
      <h2 class="title">Framework</h2>
      <div class="hero-body">
        <center>
          <img type="image/jpeg"  src="static/images/framework.jpg" width="80% />
        </center>
        
        
        <h3>Reflection module</h3>
        <br>
        <center>
          <embed type="image/svg+xml" src="static/images/dilu-reflection.svg" width="80%" />
        </center>

        <br>
        <h2 class="subtitle has-text-justified" style="padding-top: 1rem;" style="width: 80%;">
          OASim focuses on generating high-fidelity and customizable autonomous driving data through neural implicit reconstruction and rendering techniques. 
          The pipeline of OASim is shown as in the figure above. The hierarchical structure can be divided into four layers. 
          The first data layer converts the input data into the format we require, including data cleaning and labeling. The processed sensory data and labeled HD map are then input into the back-end layer. 
          This layer is the core of the system and implements 3D reconstruction, traffic flow simulation and novel data synthesis. 
          The front-end layer provides an interactive interface for users to conveniently change the vehicle route and sensor configurations. 
          The newly synthesized data can be used for multiple downstream tasks such as perception, planning, etc.

        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <h2 class="title">Sensor Simulator</h2>
      <div class="hero-body">
        <center>
          <video poster="" id="tree" autoplay controls muted loop height="100%" width="80%">
            <!-- Your video here -->
            <source src="static/videos/oasim_demo_sensor.mp4" type="video/mp4">
          </video>
        </center>
          <h2 class="subtitle has-text-centered">
            OASim allows flexible configuration of the agent's sensor suite, including cameras, LiDAR, Radar, etc. We support modifying the sensor model by changing the intrinsic and extrinsic parameters. 
            The position and orientation of the sensor relative to the vehicle body are represented by extrinsic parameters. Some commonly used intrinsic combinations are preset in the system to facilitate user selection. Once the sensor configuration is complete, it will be used to generate and preview data.
          </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <h2 class="title">3D Preview</h2>
      <div class="hero-body">
        <center>
          <video poster="" id="tree" autoplay controls muted loop height="100%" width="80%">
            <!-- Your video here -->
            <source src="static/videos/3d_preview.mp4" type="video/mp4">
          </video>
        </center>
          <h2 class="subtitle has-text-centered">
            OASim can preview the results of 3D implicit reconstruction in real time through the keyboard, and also supports conversion to 3D mesh for preview.
          </h2>
      </div>
    </div>
  </section>
  
  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <div style="position: relative;">
        <button id="copyButton" style="position: absolute; top: 0; right: 0;">Copy</button>
        <pre><code id="bibtexCode">@article{guo2023streetsurf,
  title = {StreetSurf: Extending Multi-view Implicit Surface Reconstruction to Street Views},
  author = {Guo, Jianfei and Deng, Nianchen and Li, Xinyang and Bai, Yeqi and Shi, Botian and Wang, Chiyu and Ding, Chenjing and Wang, Dongliang and Li, Yikang},
  journal = {arXiv preprint arXiv:2306.04988},
  year = {2023}
}

@misc{wen2023limsim,
  title={LimSim: A Long-term Interactive Multi-scenario Traffic Simulator}, 
  author={Licheng Wen and Daocheng Fu and Song Mao and Pinlong Cai and Min Dou and Yikang Li and Yu Qiao},
  year={2023},
  eprint={2307.06648},
  archivePrefix={arXiv},
  primaryClass={eess.SY}
}
      </code></pre>
      </div>
    </div>
  </section>

  <script>
    var copyButton = document.getElementById("copyButton");

    copyButton.addEventListener("click", function () {
      var codeElement = document.getElementById("bibtexCode");
      var textToCopy = codeElement.textContent;

      var textarea = document.createElement("textarea");
      textarea.value = textToCopy;
      document.body.appendChild(textarea);
      textarea.select();
      document.execCommand("copy");
      document.body.removeChild(textarea);

      copyButton.textContent = "✓ Copied!";

      setTimeout(function () {
        copyButton.textContent = "Copy";
      }, 2000); // Reset the button text after 2 seconds (adjust as needed)
    });
  </script>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p style="text-align: center;"><i class="fa-solid fa-heart fa-beat-fade" style="color: #ff8787;"></i>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> <i class="fa-solid fa-heart fa-beat-fade"
                style="color: #ff8787;"></i>
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>
<script>
  function getText(url, callback) {
    $.get(url, function (data) {
      callback(data);
    })
  }
  function isDecisionFrameValid(value) {
    if (value === '') {
      alert('Please input a Decision Frame.')
      return false;
    }

    var intValue = parseInt(value, 10);
    if (intValue >= 0 && intValue <= 29 && value == intValue) {
      return true;
    } else {
      alert('The Decision Frame should be an integer between 0 and 29.')
      return false;
    }
  }
  function viewFrame() {
    var currentDecisionFrame = $('#decisionFrameValue').val();
    if (isDecisionFrameValid(currentDecisionFrame)) {
      var imagePath = 'static/images/decisionFrame_' + currentDecisionFrame + '.png';
      $('#frameImage').attr('src', imagePath);
      var SDPath = 'static/text/scenarioDescription_' + currentDecisionFrame + '.txt';
      getText(SDPath, function (data) {
        var htmlText = marked.parse(data);
        $('#showDescription').html(htmlText);
      });
      var TDPath = 'static/text/thoughtsDecision_' + currentDecisionFrame + '.txt';
      getText(TDPath, function (data) {
        var htmlText = marked.parse(data);
        $('#showThoughtsDecision').html(htmlText);
      });
    }
  }
  function lastFrame() {
    var currentDecisionFrame = parseInt($('#decisionFrameValue').val(), 10);
    if (isDecisionFrameValid(currentDecisionFrame)) {
      var lastDecisionFrame = currentDecisionFrame - 1;
      if (isDecisionFrameValid(lastDecisionFrame)) {
        $("#decisionFrameValue").val(lastDecisionFrame);
        viewFrame();
      }
    }
  }
  function nextFrame() {
    var currentDecisionFrame = parseInt($('#decisionFrameValue').val(), 10);
    if (isDecisionFrameValid(currentDecisionFrame)) {
      var nextDecisionFrame = currentDecisionFrame + 1;
      if (isDecisionFrameValid(nextDecisionFrame)) {
        $("#decisionFrameValue").val(nextDecisionFrame);
        viewFrame();
      }
    }
  }
  $(document).ready(function () {
    $("#dropdownList li").click(function () {
      var number = parseInt($(this).text(), 10);
      $("#decisionFrameValue").val(number);
    });
    getText('static/text/scenarioDescription_0.txt', function (data) {
      var htmlText = marked.parse(data);
      $('#showDescription').html(htmlText);
    });
    getText('static/text/thoughtsDecision_0.txt', function (data) {
      var htmlText = marked.parse(data);
      $('#showThoughtsDecision').html(htmlText);
    })
  });

</script>

</html>